{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_u4bbsmAvxw"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "pQwAiT4mAxwQ",
        "outputId": "e22c725d-d41b-4362-9933-25166a0a11c7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UCD3mwh0AzOg",
        "outputId": "ca226259-1d8b-4e5e-8c49-a8b0b13d8fa1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JHlpvEeqdSkn",
        "outputId": "fb6715b2-6318-4ee1-db28-6a274010dc54"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def load_glove_embeddings(glove_file, word_to_index, embedding_dim):\n",
        "    \"\"\" Load GloVe embeddings and create an embedding matrix. \"\"\"\n",
        "    embeddings = {}\n",
        "    with open(glove_file, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            embeddings[word] = vector\n",
        "\n",
        "    embedding_matrix = np.zeros((len(word_to_index), embedding_dim))\n",
        "    for word, index in word_to_index.items():\n",
        "        vector = embeddings.get(word)\n",
        "        if vector is not None:\n",
        "            embedding_matrix[index] = vector\n",
        "\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "V7LhdHSSVszj",
        "outputId": "558b032c-fb98-4b0c-effe-28f1eca61812"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,pretrained_embeddings,drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        # Embedding layer with pre-trained embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(pretrained_embeddings, requires_grad=True)\n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        #print(embeds.shape)  #[50, 500, 1000]\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yCqWYE0u72w4",
        "outputId": "aa297720-37c5-4c0c-9006-f86ffee075af"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Preprocess the data\n",
        "import os\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "def read_txt_files(folder_path, label):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "            review = file.read()\n",
        "            data.append((review, label))\n",
        "    return data\n",
        "\n",
        "# Path to the folders containing positive and negative reviews\n",
        "positive_folder = '/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3 /IMDB_reviews/aclImdb/train/pos'\n",
        "negative_folder = '/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3 /IMDB_reviews/aclImdb/train/neg'\n",
        "\n",
        "positive_folder_2 = '/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3 /IMDB_reviews/aclImdb/test/pos'\n",
        "negative_folder_2 = '/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3 /IMDB_reviews/aclImdb/test/neg'\n",
        "\n",
        "# Read positive and negative reviews\n",
        "positive_data = read_txt_files(positive_folder, label=1)\n",
        "negative_data = read_txt_files(negative_folder, label=0)\n",
        "positive_data_2 = read_txt_files(positive_folder_2, label=1)\n",
        "negative_data_2 = read_txt_files(negative_folder_2, label=0)\n",
        "\n",
        "# Combine positive and negative data\n",
        "all_data = positive_data + negative_data + positive_data_2 + negative_data_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mhEWPMFkeom7",
        "outputId": "db554319-cf57-4fa8-e127-dff17f16fe25"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "os.chdir('/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3 ')\n",
        "\n",
        "with open(\"all_data_IMDB.json\", \"w\") as outfile:\n",
        "    json.dump(all_data, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "juGEZ5bEf5sl",
        "outputId": "64f8d8e6-43eb-410f-9419-5da36c1876b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "f = open('/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3/all_data_IMDB.json')\n",
        "all_data = json.load(f)\n",
        "\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6ScOECeMp9sp",
        "outputId": "0c846742-62dc-491b-cf29-9f57ffadddcf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive reviews: 25000\n",
            "Negative reviews: 25000\n"
          ]
        }
      ],
      "source": [
        "count1 = 0\n",
        "count2 = 0\n",
        "\n",
        "for item in range(0, len(all_data)):\n",
        "  if all_data[item][1] == 1:\n",
        "    count1 += 1\n",
        "  else:\n",
        "    count2 += 1\n",
        "\n",
        "print(\"Positive reviews:\", count1)\n",
        "print(\"Negative reviews:\", count2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "8LbaE7daEcw9",
        "outputId": "6648925d-c981-41ee-8f0b-23ab4927b1c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(all_data[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "SN5mPLc8IqVb",
        "outputId": "1de214d2-f864-497e-ffd4-85281e2a092a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "50000\n",
            "[\"Being the prototype of the classical Errol Flynn adventure movie and having a good story as well as two more brilliant co-stars in Maureen O'Hara (what an exquisite beauty!) and Anthony Quinn, I can only recommend this movie to all those having even the slightest liking for romance and adventure.<br /><br />Hollywood at its best!\", 1]\n"
          ]
        }
      ],
      "source": [
        "print(type(all_data))\n",
        "print(len(all_data))\n",
        "print(all_data[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to clean html tags from a sentence\n",
        "import re\n",
        "def clean_html(sentence):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    cleaned_text = re.sub(pattern,' ',sentence)\n",
        "    return cleaned_text\n",
        "\n",
        "# Function to keep only words containing letters A-Z and a-z.\n",
        "# this will remove all punctuations, special characters.\n",
        "def rem_pun(sentence):\n",
        "    cleaned_text  = re.sub('[^a-zA-Z]',' ',sentence)\n",
        "    return (cleaned_text)\n",
        "\n",
        "#Remove URL from sentences.\n",
        "def rem_url(sen):\n",
        "    txt = re.sub(r\"http\\S+\", \" \", sen)\n",
        "    sen = re.sub(r\"www.\\S+\", \" \", txt)\n",
        "    return (sen)\n",
        "\n",
        "#Remove words like 'ddddddddd', 'funnnnnn', 'coolllllll' etc. Preserves words like 'goods', 'cool', 'best' etc. We will remove all such words which has three consecutive repeating characters.\n",
        "def remove_extra(sen):\n",
        "    cleaned_text  = re.sub(\"\\s*\\b(?=\\w*(\\w)\\1{2,})\\w*\\b\",' ',sen)\n",
        "    return (cleaned_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "eKKNH9Q-4gnJ",
        "outputId": "c2f1eee9-beca-44e3-840a-1aa59b8ea9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_LF6cFdv2bW0",
        "outputId": "055caf2e-f570-42d4-aa0b-55be26f7ca33"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import random\n",
        "random.shuffle(all_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "aBxt9O7f2xft",
        "outputId": "71fa6727-2036-40c2-8560-9c6a22407c90"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Honestly, when I went to see this movie at the Rave theater in Plainfield Indiana, I did not expect much. I went to this movie only because I figured hey, it's a WWE movie it'll be good for a laugh. Then I sat down and watched it and saw why they chose Glen Jacobs (Kane) to play Jacob Goodnight. He is probably one of the freakiest guys on the big screen (much worse in my opinion than Freddy or Jason) and has one big advantage to other movies that attracts me to a horror movie. It shows Jacob Goodnight as someone who is human. He has a heart, no matter how twisted and creepy it is. He feels pain, something that Jason never does or appears to show. He feels sorrow and pleasure, though again both of them insane which you will notice if you see the movie. All in all, a different experience in my opinion than many slashers, and it surprised me in a few ways, as in who lived in the end.\", 1]\n"
          ]
        }
      ],
      "source": [
        "print(all_data[50])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_data[0])\n",
        "print()\n",
        "print(all_data[1])\n",
        "print()\n",
        "print(all_data[2])\n",
        "print()\n",
        "print(all_data[3])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "UWeyW_ZY5jf0",
        "outputId": "e491470e-6181-4717-9a3a-7502eac4b1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The excruciatingly slow pace of this film was probably the director's express intention, in order to convey what life was like growing up as a village teen in China. However, I found the combination of the glacially slow 'plot' and the general filming style so impersonal as to be totally alienating, particularly to a western audience. At times I actually had trouble telling some characters apart, as they were filmed from such a distance. Two hours in and I was totally past caring. As someone who is not only interested in music but is also very into the history and culture of China (and is by the way no stranger to Chinese cinema), I couldn't engage with a single character and found nothing to get my teeth into. It begs the question: If I disliked it, who on earth would like it? Give me Zhang Yimou, give me Chen Kaige. Give me the work of just about any other Chinese director I've ever seen. This sorry effort just doesn't measure up at all. I'd be sorry to see Chinese cinema judged against this benchmark.\", 0]\n",
            "\n",
            "['I really enjoyed The Patriot. This movie had less violence and was based on a real life threat that could inevitably destroy our civilization. One line in the movie from wesley mclaren (seagal) stuck out in my mind to be very true of our society, \"western medicine is in the practice of prolonging illness and I am in the business of curing it.\"<br /><br />', 1]\n",
            "\n",
            "[\"In sixth grade, every teacher I had decided it would be a great idea to make this movie the curriculum for an entire semester. Every class had something to do with this terrible show. We watched it in English and wrote in journals as if we were one of the characters. In math we talked about charts and other sea crap. In science we talked about whales (which was actually somewhat interesting, so this wasn't a 100% waste of time). All day everyday was torture. Not only that, but they would subject us to this horror twice a day by making us watch it in study hall as well. I could see if this was a new series or something, but it was, like, '93. I'm still trying to block this out.\", 0]\n",
            "\n",
            "[\"Cut to the chase, this is one of the five worst films that I've ever seen.<br /><br />Not that they didn't try. There was some decent writing with some elements of structure in there, a good cast, some good acting. I'm not sure where it went wrong, but it went horribly wrong.<br /><br />Some of the elements may have been bad structure and no substantive story, a lot of overacting by the lead (who probably is much better when restrained), some bad directing and editing. I had enough at about an hour, tearing my hair out at about a hour and a half and very agitated at the hour and fifty minutes it ran. There was also an insincerity about it all, being that I went with someone who used to be a heroin addict. He was agitated that it glamorized something that had nothing good to it. That was bolstered by the pretty 17 year-old girl who was in love with the 30 year-old junkie.<br /><br />And the frantic nature of the lead was a turn-off enough. There were clunky plot points that were an attempt at a structure, but the end result was listless and unending (with uneven time lines). The characters were colorful but to no end, which made me feel bad for the quality actors who you've just not seen enough.<br /><br />Skip it. I assumed that this was a first-time director who was enamored by his own turds, but he has done this before. I'm puzzled how this and many other really bad ideas find someone who will actually give them money.\", 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = []\n",
        "\n",
        "for sentence in range(0, len(all_data)):\n",
        "\n",
        "  sent = all_data[sentence][0]\n",
        "\n",
        "  sent = clean_html(sent)\n",
        "  sent = rem_url(sent)\n",
        "  all_data[sentence][0] = sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "VoRMhvMB4kfg",
        "outputId": "3a59f2b1-c725-430e-de6e-4ebe9e651ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_data[0])\n",
        "print()\n",
        "print(all_data[1])\n",
        "print()\n",
        "print(all_data[2])\n",
        "print()\n",
        "print(all_data[3])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "uWes08zW5m0s",
        "outputId": "d3623ab3-6768-4efd-d505-e6c0fa246a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The excruciatingly slow pace of this film was probably the director's express intention, in order to convey what life was like growing up as a village teen in China. However, I found the combination of the glacially slow 'plot' and the general filming style so impersonal as to be totally alienating, particularly to a western audience. At times I actually had trouble telling some characters apart, as they were filmed from such a distance. Two hours in and I was totally past caring. As someone who is not only interested in music but is also very into the history and culture of China (and is by the way no stranger to Chinese cinema), I couldn't engage with a single character and found nothing to get my teeth into. It begs the question: If I disliked it, who on earth would like it? Give me Zhang Yimou, give me Chen Kaige. Give me the work of just about any other Chinese director I've ever seen. This sorry effort just doesn't measure up at all. I'd be sorry to see Chinese cinema judged against this benchmark.\", 0]\n",
            "\n",
            "['I really enjoyed The Patriot. This movie had less violence and was based on a real life threat that could inevitably destroy our civilization. One line in the movie from wesley mclaren (seagal) stuck out in my mind to be very true of our society, \"western medicine is in the practice of prolonging illness and I am in the business of curing it.\"  ', 1]\n",
            "\n",
            "[\"In sixth grade, every teacher I had decided it would be a great idea to make this movie the curriculum for an entire semester. Every class had something to do with this terrible show. We watched it in English and wrote in journals as if we were one of the characters. In math we talked about charts and other sea crap. In science we talked about whales (which was actually somewhat interesting, so this wasn't a 100% waste of time). All day everyday was torture. Not only that, but they would subject us to this horror twice a day by making us watch it in study hall as well. I could see if this was a new series or something, but it was, like, '93. I'm still trying to block this out.\", 0]\n",
            "\n",
            "[\"Cut to the chase, this is one of the five worst films that I've ever seen.  Not that they didn't try. There was some decent writing with some elements of structure in there, a good cast, some good acting. I'm not sure where it went wrong, but it went horribly wrong.  Some of the elements may have been bad structure and no substantive story, a lot of overacting by the lead (who probably is much better when restrained), some bad directing and editing. I had enough at about an hour, tearing my hair out at about a hour and a half and very agitated at the hour and fifty minutes it ran. There was also an insincerity about it all, being that I went with someone who used to be a heroin addict. He was agitated that it glamorized something that had nothing good to it. That was bolstered by the pretty 17 year-old girl who was in love with the 30 year-old junkie.  And the frantic nature of the lead was a turn-off enough. There were clunky plot points that were an attempt at a structure, but the end result was listless and unending (with uneven time lines). The characters were colorful but to no end, which made me feel bad for the quality actors who you've just not seen enough.  Skip it. I assumed that this was a first-time director who was enamored by his own turds, but he has done this before. I'm puzzled how this and many other really bad ideas find someone who will actually give them money.\", 0]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert all the words to lower case\n",
        "#Source https://github.com/saugatapaul1010/Amazon-Fine-Food-Reviews-Analysis\n",
        "import re\n",
        "\n",
        "def lower_case(x):\n",
        "    x = str(x).lower()\n",
        "    x = x.replace(\",000,000\", \" m\").replace(\",000\", \" k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
        "                           .replace(\"won't\", \" will not\").replace(\"cannot\", \" can not\").replace(\"can't\", \" can not\")\\\n",
        "                           .replace(\"n't\", \" not\").replace(\"what's\", \" what is\").replace(\"it's\", \" it is\")\\\n",
        "                           .replace(\"'ve\", \" have\").replace(\"'m\", \" am\").replace(\"'re\", \" are\")\\\n",
        "                           .replace(\"he's\", \" he is\").replace(\"she's\", \" she is\").replace(\"'s\", \" own\")\\\n",
        "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
        "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"how's\",\" how has\").replace(\"y'all\",\" you all\")\\\n",
        "                           .replace(\"o'clock\",\" of the clock\").replace(\"ne'er\",\" never\").replace(\"let's\",\" let us\")\\\n",
        "                           .replace(\"finna\",\" fixing to\").replace(\"gonna\",\" going to\").replace(\"gimme\",\" give me\").replace(\"gotta\",\" got to\").replace(\"'d\",\" would\")\\\n",
        "                           .replace(\"daresn't\",\" dare not\").replace(\"dasn't\",\" dare not\").replace(\"e'er\",\" ever\").replace(\"everyone's\",\" everyone is\")\\\n",
        "                           .replace(\"'cause'\",\" because\")\n",
        "\n",
        "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
        "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
        "    return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2vdeL33r4-Gg",
        "outputId": "b448be93-bb31-4dde-ebee-63c5f683389b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZevhyqXs88G6",
        "outputId": "4486c9f6-ab90-4fc7-a7da-99264de167d4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147682\n",
            "Sample original text: The excruciatingly slow pace of this film was probably the director's express intention, in order to convey what life was like growing up as a village teen in China. However, I found the combination of the glacially slow 'plot' and the general filming style so impersonal as to be totally alienating, particularly to a western audience. At times I actually had trouble telling some characters apart, as they were filmed from such a distance. Two hours in and I was totally past caring. As someone who is not only interested in music but is also very into the history and culture of China (and is by the way no stranger to Chinese cinema), I couldn't engage with a single character and found nothing to get my teeth into. It begs the question: If I disliked it, who on earth would like it? Give me Zhang Yimou, give me Chen Kaige. Give me the work of just about any other Chinese director I've ever seen. This sorry effort just doesn't measure up at all. I'd be sorry to see Chinese cinema judged against this benchmark.\n",
            "Sample tokenized text: ['the', 'excruciatingly', 'slow', 'pace', 'of', 'this', 'film', 'was', 'probably', 'the', 'director', \"'\", 's', 'express', 'intention', ',', 'in', 'order', 'to', 'convey', 'what', 'life', 'was', 'like', 'growing', 'up', 'as', 'a', 'village', 'teen', 'in', 'china', '.', 'however', ',', 'i', 'found', 'the', 'combination', 'of', 'the', 'glacially', 'slow', \"'\", 'plot', \"'\", 'and', 'the', 'general', 'filming', 'style', 'so', 'impersonal', 'as', 'to', 'be', 'totally', 'alienating', ',', 'particularly', 'to', 'a', 'western', 'audience', '.', 'at', 'times', 'i', 'actually', 'had', 'trouble', 'telling', 'some', 'characters', 'apart', ',', 'as', 'they', 'were', 'filmed', 'from', 'such', 'a', 'distance', '.', 'two', 'hours', 'in', 'and', 'i', 'was', 'totally', 'past', 'caring', '.', 'as', 'someone', 'who', 'is', 'not', 'only', 'interested', 'in', 'music', 'but', 'is', 'also', 'very', 'into', 'the', 'history', 'and', 'culture', 'of', 'china', '(', 'and', 'is', 'by', 'the', 'way', 'no', 'stranger', 'to', 'chinese', 'cinema', ')', ',', 'i', 'couldn', \"'\", 't', 'engage', 'with', 'a', 'single', 'character', 'and', 'found', 'nothing', 'to', 'get', 'my', 'teeth', 'into', '.', 'it', 'begs', 'the', 'question', 'if', 'i', 'disliked', 'it', ',', 'who', 'on', 'earth', 'would', 'like', 'it', '?', 'give', 'me', 'zhang', 'yimou', ',', 'give', 'me', 'chen', 'kaige', '.', 'give', 'me', 'the', 'work', 'of', 'just', 'about', 'any', 'other', 'chinese', 'director', 'i', \"'\", 've', 'ever', 'seen', '.', 'this', 'sorry', 'effort', 'just', 'doesn', \"'\", 't', 'measure', 'up', 'at', 'all', '.', 'i', \"'\", 'd', 'be', 'sorry', 'to', 'see', 'chinese', 'cinema', 'judged', 'against', 'this', 'benchmark', '.']\n",
            "Sample numericalized text: [1, 7233, 608, 1023, 6, 12, 22, 15, 233, 1, 168, 48, 308, 2543, 3397, 3, 10, 623, 7, 2972, 54, 128, 15, 42, 1824, 66, 16, 5, 1970, 1501, 10, 2479, 2, 190, 3, 11, 248, 1, 2104, 6, 1, 50420, 608, 48, 123, 48, 4, 1, 786, 1340, 427, 41, 21355, 16, 7, 31, 446, 15631, 3, 569, 7, 5, 885, 297, 2, 34, 205, 11, 162, 75, 1075, 1014, 55, 110, 952, 3, 16, 36, 76, 729, 40, 146, 5, 3739, 2, 117, 610, 10, 4, 11, 15, 446, 493, 2900, 2, 16, 276, 39, 8, 14, 70, 880, 10, 204, 21, 8, 92, 61, 93, 1, 472, 4, 1168, 6, 2479, 24, 4, 8, 37, 1, 103, 65, 2603, 7, 1705, 428, 23, 3, 11, 0, 48, 1180, 4696, 17, 5, 682, 113, 4, 248, 163, 7, 86, 63, 2675, 93, 2, 9, 7530, 1, 866, 50, 11, 4928, 9, 3, 39, 27, 688, 57, 42, 9, 53, 196, 77, 8939, 16619, 3, 196, 77, 9551, 26902, 2, 196, 77, 1, 165, 6, 45, 49, 108, 88, 1705, 168, 11, 48, 27503, 129, 116, 2, 12, 734, 745, 45, 60011, 48, 1180, 3959, 66, 34, 35, 2, 11, 48, 1056, 31, 734, 7, 73, 1705, 428, 6864, 441, 12, 15647, 2]\n",
            "Sample label: 0\n",
            "Sample original text: This is definitely one of the ultimate cult classics, and is a must see for all psychotronic fans. Why? It has everything a great 70s exploitation film should have. Over-the-top dialog, bad acting, enthusiasm, sex, sleaze, political incorrectness, violence, and many other elements of a good cult classic are included. In other words, Dolemite is a must-see.  As with a lot of these films, the plot makes little to no sense. What I picked up from it is that pimp-hustler Dolemite got framed up for having stolen furs and half a million dollars worth of narcotics. While he was doing time, his arch nemesis Willie Green (the same man who framed him) took over his nightclub. However, the sympathetic warden (the only white character in the whole movie that isn't completely evil or incompetent) decides to spring him free to stop the evil Willie Green and his drug trafficking. Luckily, he knows kung fu, as does about 50 to 75% of the characters in this film do. And even more luckily, while he was locked up, the madam Queen Bee sent all his \"hoes\" to kung fu school. With this army of kung fu fighting \"hoes\" (his words, not mine) on his side, he plans to take back the nightclub from Willie Green. However, two racist white cops try to frame him up again and have him thrown back in jail.  As I said earlier, don't try to follow the plot. I've seen this movie about five times and there are many elements that seem to have no connections to anything else. Supporting characters wander in and out of the film. I'm still attempting to figure out what was up with Reverend Gibbs, the Mayor, and the Hamburger Pimp. Who cares ultimately? The scenes with these characters are all priceless. As for the dialog, its horrible with even worse delivery. Since Rudy Ray Moore was originally a comedian, I begin to wonder if this film was meant to be a spoof or a serious action film. It seems he couldn't decide which one. Lines such as \"Yeah, I'm so bad, I kick my own ass twice a day\" call for further investigation. Either way, the film is hilarious, and the plot has more holes than a swiss cheese factory. Another hilarious element is some of the most unerotic uses of sex and nudity ever in film. Actors that you would never want to see naked get naked (including the Mayor and Queen Bee). Not to mention the fact that the boom mic seems to show up in every other scene.  Most of all, Moore shows incredible enthusiasm. He seems to be having a generally good time and is certainly charismatic. His comedy raps proved to be a huge influence on latter day gangsta rap, including Dr. Dre who sampled him on his groundbreaking 1992 album \"The Chronic\". As technically inept as the film is, it is culturally influential. Even more important, it is an all around good time. The biggest crime an exploitation film can commit is being boring, and this for all its flaws is quickly paced and entertaining. In other words, if you dig this kind of film, you'll love \"Dolemite\". If you don't dig it, you're a \"no-business, born-insecure, jock-jawed motha-f***a!\" (7/10)\n",
            "Sample tokenized text: ['this', 'is', 'definitely', 'one', 'of', 'the', 'ultimate', 'cult', 'classics', ',', 'and', 'is', 'a', 'must', 'see', 'for', 'all', 'psychotronic', 'fans', '.', 'why', '?', 'it', 'has', 'everything', 'a', 'great', '70s', 'exploitation', 'film', 'should', 'have', '.', 'over-the-top', 'dialog', ',', 'bad', 'acting', ',', 'enthusiasm', ',', 'sex', ',', 'sleaze', ',', 'political', 'incorrectness', ',', 'violence', ',', 'and', 'many', 'other', 'elements', 'of', 'a', 'good', 'cult', 'classic', 'are', 'included', '.', 'in', 'other', 'words', ',', 'dolemite', 'is', 'a', 'must-see', '.', 'as', 'with', 'a', 'lot', 'of', 'these', 'films', ',', 'the', 'plot', 'makes', 'little', 'to', 'no', 'sense', '.', 'what', 'i', 'picked', 'up', 'from', 'it', 'is', 'that', 'pimp-hustler', 'dolemite', 'got', 'framed', 'up', 'for', 'having', 'stolen', 'furs', 'and', 'half', 'a', 'million', 'dollars', 'worth', 'of', 'narcotics', '.', 'while', 'he', 'was', 'doing', 'time', ',', 'his', 'arch', 'nemesis', 'willie', 'green', '(', 'the', 'same', 'man', 'who', 'framed', 'him', ')', 'took', 'over', 'his', 'nightclub', '.', 'however', ',', 'the', 'sympathetic', 'warden', '(', 'the', 'only', 'white', 'character', 'in', 'the', 'whole', 'movie', 'that', 'isn', \"'\", 't', 'completely', 'evil', 'or', 'incompetent', ')', 'decides', 'to', 'spring', 'him', 'free', 'to', 'stop', 'the', 'evil', 'willie', 'green', 'and', 'his', 'drug', 'trafficking', '.', 'luckily', ',', 'he', 'knows', 'kung', 'fu', ',', 'as', 'does', 'about', '50', 'to', '75%', 'of', 'the', 'characters', 'in', 'this', 'film', 'do', '.', 'and', 'even', 'more', 'luckily', ',', 'while', 'he', 'was', 'locked', 'up', ',', 'the', 'madam', 'queen', 'bee', 'sent', 'all', 'his', 'hoes', 'to', 'kung', 'fu', 'school', '.', 'with', 'this', 'army', 'of', 'kung', 'fu', 'fighting', 'hoes', '(', 'his', 'words', ',', 'not', 'mine', ')', 'on', 'his', 'side', ',', 'he', 'plans', 'to', 'take', 'back', 'the', 'nightclub', 'from', 'willie', 'green', '.', 'however', ',', 'two', 'racist', 'white', 'cops', 'try', 'to', 'frame', 'him', 'up', 'again', 'and', 'have', 'him', 'thrown', 'back', 'in', 'jail', '.', 'as', 'i', 'said', 'earlier', ',', 'don', \"'\", 't', 'try', 'to', 'follow', 'the', 'plot', '.', 'i', \"'\", 've', 'seen', 'this', 'movie', 'about', 'five', 'times', 'and', 'there', 'are', 'many', 'elements', 'that', 'seem', 'to', 'have', 'no', 'connections', 'to', 'anything', 'else', '.', 'supporting', 'characters', 'wander', 'in', 'and', 'out', 'of', 'the', 'film', '.', 'i', \"'\", 'm', 'still', 'attempting', 'to', 'figure', 'out', 'what', 'was', 'up', 'with', 'reverend', 'gibbs', ',', 'the', 'mayor', ',', 'and', 'the', 'hamburger', 'pimp', '.', 'who', 'cares', 'ultimately', '?', 'the', 'scenes', 'with', 'these', 'characters', 'are', 'all', 'priceless', '.', 'as', 'for', 'the', 'dialog', ',', 'its', 'horrible', 'with', 'even', 'worse', 'delivery', '.', 'since', 'rudy', 'ray', 'moore', 'was', 'originally', 'a', 'comedian', ',', 'i', 'begin', 'to', 'wonder', 'if', 'this', 'film', 'was', 'meant', 'to', 'be', 'a', 'spoof', 'or', 'a', 'serious', 'action', 'film', '.', 'it', 'seems', 'he', 'couldn', \"'\", 't', 'decide', 'which', 'one', '.', 'lines', 'such', 'as', 'yeah', ',', 'i', \"'\", 'm', 'so', 'bad', ',', 'i', 'kick', 'my', 'own', 'ass', 'twice', 'a', 'day', 'call', 'for', 'further', 'investigation', '.', 'either', 'way', ',', 'the', 'film', 'is', 'hilarious', ',', 'and', 'the', 'plot', 'has', 'more', 'holes', 'than', 'a', 'swiss', 'cheese', 'factory', '.', 'another', 'hilarious', 'element', 'is', 'some', 'of', 'the', 'most', 'unerotic', 'uses', 'of', 'sex', 'and', 'nudity', 'ever', 'in', 'film', '.', 'actors', 'that', 'you', 'would', 'never', 'want', 'to', 'see', 'naked', 'get', 'naked', '(', 'including', 'the', 'mayor', 'and', 'queen', 'bee', ')', '.', 'not', 'to', 'mention', 'the', 'fact', 'that', 'the', 'boom', 'mic', 'seems', 'to', 'show', 'up', 'in', 'every', 'other', 'scene', '.', 'most', 'of', 'all', ',', 'moore', 'shows', 'incredible', 'enthusiasm', '.', 'he', 'seems', 'to', 'be', 'having', 'a', 'generally', 'good', 'time', 'and', 'is', 'certainly', 'charismatic', '.', 'his', 'comedy', 'raps', 'proved', 'to', 'be', 'a', 'huge', 'influence', 'on', 'latter', 'day', 'gangsta', 'rap', ',', 'including', 'dr', '.', 'dre', 'who', 'sampled', 'him', 'on', 'his', 'groundbreaking', '1992', 'album', 'the', 'chronic', '.', 'as', 'technically', 'inept', 'as', 'the', 'film', 'is', ',', 'it', 'is', 'culturally', 'influential', '.', 'even', 'more', 'important', ',', 'it', 'is', 'an', 'all', 'around', 'good', 'time', '.', 'the', 'biggest', 'crime', 'an', 'exploitation', 'film', 'can', 'commit', 'is', 'being', 'boring', ',', 'and', 'this', 'for', 'all', 'its', 'flaws', 'is', 'quickly', 'paced', 'and', 'entertaining', '.', 'in', 'other', 'words', ',', 'if', 'you', 'dig', 'this', 'kind', 'of', 'film', ',', 'you', \"'\", 'll', 'love', 'dolemite', '.', 'if', 'you', 'don', \"'\", 't', 'dig', 'it', ',', 'you', \"'\", 're', 'a', 'no-business', ',', 'born-insecure', ',', 'jock-jawed', 'motha-f***a', '!', '(', '7/10', ')']\n",
            "Sample numericalized text: [12, 8, 390, 32, 6, 1, 2040, 1260, 2031, 3, 4, 8, 5, 211, 73, 18, 35, 67301, 447, 2, 141, 53, 9, 51, 258, 5, 91, 1850, 2321, 22, 142, 28, 2, 2493, 699, 3, 89, 121, 3, 4737, 3, 380, 3, 5004, 3, 983, 22854, 3, 564, 3, 4, 115, 88, 776, 6, 5, 58, 1260, 351, 26, 1884, 2, 10, 88, 655, 3, 8635, 8, 5, 3688, 2, 16, 17, 5, 169, 6, 140, 114, 3, 1, 123, 164, 127, 7, 65, 278, 2, 54, 11, 1572, 66, 40, 9, 8, 13, 123381, 8635, 183, 5738, 66, 18, 254, 2545, 25558, 4, 349, 5, 1458, 2177, 270, 6, 14936, 2, 144, 29, 15, 391, 68, 3, 30, 11229, 5234, 5219, 1405, 24, 1, 170, 132, 39, 5738, 96, 23, 534, 138, 30, 5679, 2, 190, 3, 1, 2165, 6185, 24, 1, 70, 471, 113, 10, 1, 218, 19, 13, 0, 48, 1180, 323, 463, 44, 4557, 23, 1049, 7, 4234, 96, 929, 7, 577, 1, 463, 5219, 1405, 4, 30, 1237, 14423, 2, 3467, 3, 29, 654, 3090, 2776, 3, 16, 80, 49, 1228, 7, 0, 6, 1, 110, 10, 12, 22, 46, 2, 4, 64, 60, 3467, 3, 144, 29, 15, 3327, 66, 3, 1, 17621, 1610, 10439, 1388, 35, 30, 44278, 7, 3090, 2776, 368, 2, 17, 12, 1210, 6, 3090, 2776, 987, 44278, 24, 30, 655, 3, 14, 1747, 23, 27, 30, 497, 3, 29, 2277, 7, 191, 149, 1, 5679, 40, 5219, 1405, 2, 190, 3, 117, 2764, 471, 1464, 344, 7, 2064, 96, 66, 175, 4, 28, 96, 1300, 149, 10, 2628, 2, 16, 11, 296, 915, 3, 1612, 48, 1180, 344, 7, 839, 1, 123, 2, 11, 48, 27503, 116, 12, 19, 49, 724, 205, 4, 43, 26, 115, 776, 13, 299, 7, 28, 65, 5667, 7, 229, 314, 2, 713, 110, 6184, 10, 4, 52, 6, 1, 22, 2, 11, 48, 2307, 137, 2759, 7, 814, 52, 54, 15, 66, 17, 11669, 26755, 3, 1, 4433, 3, 4, 1, 16765, 7349, 2, 39, 2095, 1116, 53, 1, 143, 17, 140, 110, 26, 35, 4449, 2, 16, 18, 1, 699, 3, 99, 480, 17, 64, 419, 2741, 2, 230, 7875, 1291, 1753, 15, 1830, 5, 2863, 3, 11, 862, 7, 560, 50, 12, 22, 15, 975, 7, 31, 5, 2564, 44, 5, 587, 221, 22, 2, 9, 188, 29, 0, 48, 1180, 1120, 69, 32, 2, 402, 146, 16, 1090, 3, 11, 48, 2307, 41, 89, 3, 11, 2089, 63, 20, 2276, 1429, 5, 255, 633, 18, 1029, 3373, 2, 335, 103, 3, 1, 22, 8, 559, 3, 4, 1, 123, 51, 60, 1437, 82, 5, 8844, 2894, 3522, 2, 159, 559, 1499, 8, 55, 6, 1, 97, 30599, 1051, 6, 380, 4, 1021, 129, 10, 22, 2, 155, 13, 25, 57, 119, 181, 7, 73, 1263, 86, 1263, 24, 572, 1, 4433, 4, 1610, 10439, 23, 2, 14, 7, 712, 1, 193, 13, 1, 5245, 14927, 188, 7, 124, 66, 10, 173, 88, 136, 2, 97, 6, 35, 3, 1753, 267, 1031, 4737, 2, 29, 188, 7, 31, 254, 5, 1247, 58, 68, 4, 8, 412, 3632, 2, 30, 210, 32189, 2153, 7, 31, 5, 620, 2331, 27, 1553, 255, 13292, 4321, 3, 572, 763, 2, 39214, 39, 34612, 96, 27, 30, 7069, 7156, 4269, 1, 14508, 2, 16, 2624, 2751, 16, 1, 22, 8, 3, 9, 8, 15392, 7119, 2, 64, 60, 648, 3, 9, 8, 38, 35, 186, 58, 68, 2, 1, 1084, 820, 38, 2321, 22, 56, 3399, 8, 118, 338, 3, 4, 12, 18, 35, 99, 1495, 8, 904, 2571, 4, 421, 2, 10, 88, 655, 3, 50, 25, 3510, 12, 235, 6, 22, 3, 25, 48, 13803, 120, 8635, 2, 50, 25, 1612, 48, 1180, 3510, 9, 3, 25, 48, 7134, 5, 118095, 3, 83776, 3, 108042, 116005, 33, 24, 2332, 23]\n",
            "Sample label: 1\n",
            "Sample original text: I found it hard to care about these characters, who were either annoying or insipid, all living their fabulously hilariously urban lives.  The dialogue was excruiciating at times, and at other times the narrative seemed hard to follow - was it me or were entire scenes deleted?  It felt like a poor sitcom somehow turned into a film. The stereotypes and jokes about \"men's groups\" would perhaps have been funny in the early 90s. As it is, this is where much of the humour of the film comes from - and boy, does it get old fast.  Apart from the attractive Irish man - this film was a dud. And not even in a \"so bad it's good way\". The last 20 minutes were particularly painful. Perhaps if you've never met any gay people or never thought about homosexuality before, then this film might have something meaningful to say. Otherwise - darlings, you'd still be better off renting The Boys in The Band or Beautiful Thing.\n",
            "Sample tokenized text: ['i', 'found', 'it', 'hard', 'to', 'care', 'about', 'these', 'characters', ',', 'who', 'were', 'either', 'annoying', 'or', 'insipid', ',', 'all', 'living', 'their', 'fabulously', 'hilariously', 'urban', 'lives', '.', 'the', 'dialogue', 'was', 'excruiciating', 'at', 'times', ',', 'and', 'at', 'other', 'times', 'the', 'narrative', 'seemed', 'hard', 'to', 'follow', '-', 'was', 'it', 'me', 'or', 'were', 'entire', 'scenes', 'deleted', '?', 'it', 'felt', 'like', 'a', 'poor', 'sitcom', 'somehow', 'turned', 'into', 'a', 'film', '.', 'the', 'stereotypes', 'and', 'jokes', 'about', 'men', \"'\", 's', 'groups', 'would', 'perhaps', 'have', 'been', 'funny', 'in', 'the', 'early', '90s', '.', 'as', 'it', 'is', ',', 'this', 'is', 'where', 'much', 'of', 'the', 'humour', 'of', 'the', 'film', 'comes', 'from', '-', 'and', 'boy', ',', 'does', 'it', 'get', 'old', 'fast', '.', 'apart', 'from', 'the', 'attractive', 'irish', 'man', '-', 'this', 'film', 'was', 'a', 'dud', '.', 'and', 'not', 'even', 'in', 'a', 'so', 'bad', 'it', \"'\", 's', 'good', 'way', '.', 'the', 'last', '20', 'minutes', 'were', 'particularly', 'painful', '.', 'perhaps', 'if', 'you', \"'\", 've', 'never', 'met', 'any', 'gay', 'people', 'or', 'never', 'thought', 'about', 'homosexuality', 'before', ',', 'then', 'this', 'film', 'might', 'have', 'something', 'meaningful', 'to', 'say', '.', 'otherwise', '-', 'darlings', ',', 'you', \"'\", 'd', 'still', 'be', 'better', 'off', 'renting', 'the', 'boys', 'in', 'the', 'band', 'or', 'beautiful', 'thing', '.']\n",
            "Sample numericalized text: [11, 248, 9, 260, 7, 448, 49, 140, 110, 3, 39, 76, 335, 614, 44, 5616, 3, 35, 570, 72, 17972, 4317, 2583, 454, 2, 1, 400, 15, 60726, 34, 205, 3, 4, 34, 88, 205, 1, 1321, 451, 260, 7, 839, 84, 15, 9, 77, 44, 76, 433, 143, 5574, 53, 9, 429, 42, 5, 325, 2874, 799, 661, 93, 5, 22, 2, 1, 1949, 4, 602, 49, 333, 48, 308, 3910, 57, 376, 28, 87, 158, 10, 1, 404, 4457, 2, 16, 9, 8, 3, 12, 8, 122, 83, 6, 1, 1148, 6, 1, 22, 261, 40, 84, 4, 415, 3, 80, 9, 86, 174, 830, 2, 952, 40, 1, 1527, 2473, 132, 84, 12, 22, 15, 5, 5542, 2, 4, 14, 64, 10, 5, 41, 89, 9, 48, 308, 58, 103, 2, 1, 232, 878, 225, 76, 569, 1333, 2, 376, 50, 25, 48, 27503, 119, 1769, 108, 888, 90, 44, 119, 197, 49, 5958, 161, 3, 101, 12, 22, 228, 28, 145, 3131, 7, 139, 2, 863, 84, 21929, 3, 25, 48, 1056, 137, 31, 131, 133, 2574, 1, 848, 10, 1, 1080, 44, 303, 152, 2]\n",
            "Sample label: 0\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Tokenize the data\n",
        "tok_reviews = [tokenizer(lower_case(review)) for review, _ in all_data]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(tok_reviews, specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])  # Handle unknown tokens\n",
        "print(len(vocab))\n",
        "\n",
        "# Get the stoi (string-to-index) mapping from the vocab\n",
        "stoi = vocab.get_stoi()\n",
        "\n",
        "# Create word_to_index mapping, shifting indices by 1 and reserving 0 for <PAD>\n",
        "word_to_index = {word: (index + 1) for word, index in stoi.items()}\n",
        "word_to_index['<PAD>'] = 0\n",
        "\n",
        "def preprocess(data, vocab, max_sequence_length=512):\n",
        "    tok_reviews, labels = zip(*[(tokenizer(review), label) for review, label in data])\n",
        "\n",
        "    # Numericalize the sentences\n",
        "    numericalized_data = [[vocab[token] for token in sentence] for sentence in tok_reviews]\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequence([torch.tensor(seq[:max_sequence_length] + [0] * max(0, max_sequence_length - len(seq))) for seq in numericalized_data], batch_first=True, padding_value=0)\n",
        "\n",
        "    print(\"Sample original text:\", data[0][0])  # Print sample original text\n",
        "    print(\"Sample tokenized text:\", tok_reviews[0])  # Print sample tokenized text\n",
        "    print(\"Sample numericalized text:\", numericalized_data[0])  # Print sample numericalized text\n",
        "    print(\"Sample label:\", labels[0])  # Print sample label\n",
        "\n",
        "    return padded_sequences, torch.tensor(labels)\n",
        "\n",
        "# Split data before processing\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "train_split = int(len(all_data) * train_ratio)\n",
        "val_split = int(len(all_data) * (train_ratio + val_ratio))\n",
        "\n",
        "train_data = all_data[:train_split]\n",
        "val_data = all_data[train_split:val_split]\n",
        "test_data = all_data[val_split:]\n",
        "\n",
        "# Process the datasets\n",
        "x_train_padded, y_train_tensor = preprocess(train_data, vocab)\n",
        "x_val_padded, y_val_tensor = preprocess(val_data, vocab)\n",
        "x_test_padded, y_test_tensor = preprocess(test_data, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5-1Z7m7Jn0u"
      },
      "outputs": [],
      "source": [
        "print(x_train_padded.unsqueeze(1)[0])\n",
        "\n",
        "print(type(x_train_padded))\n",
        "print(x_train_padded[0])\n",
        "print(type(x_train_padded[0]))\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cT6hDF8hFphE",
        "outputId": "13d3af51-81ff-43b4-93b7-be3626cdd116"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "\n",
        "# x_train_tensor = x_train_padded.unsqueeze(1)\n",
        "y_train_tensor = y_train_tensor.unsqueeze(1)\n",
        "# x_val_tensor = x_val_padded.unsqueeze(1)\n",
        "y_val_tensor = y_val_tensor.unsqueeze(1)\n",
        "# x_test_tensor = x_test_padded.unsqueeze(1)\n",
        "y_test_tensor = y_test_tensor.unsqueeze(1)\n",
        "\n",
        "# Create DataLoaders directly with padded data\n",
        "train_dataset = TensorDataset(x_train_padded, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "val_dataset = TensorDataset(x_val_padded, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataset = TensorDataset(x_test_padded, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "print(len(vocab))\n",
        "embedding_dim = 200\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "\n",
        "# Assuming you have word_to_index mapping and the GloVe file path\n",
        "glove_file = 'glove.6B.200d.txt'  # Path to your GloVe file\n",
        "embedding_dim = 200  # Make sure this matches the GloVe embeddings you are using\n",
        "\n",
        "# Load GloVe embeddings\n",
        "pretrained_embeddings = load_glove_embeddings(glove_file, word_to_index, embedding_dim)\n",
        "# pretrained_embeddings.to(device)\n",
        "\n",
        "model = SentimentRNN(no_layers, vocab_size, hidden_dim, embedding_dim, pretrained_embeddings, drop_prob=0.5)\n",
        "model.to(device)\n",
        "\n",
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CPfBrd-K6Axt",
        "outputId": "2d527391-b757-4810-b0b8-1e784163c400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "noLXuaZIe9U-",
        "outputId": "05c37ee9-fd3a-48bb-8b4d-9b5d8fd0697c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([99343, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "pretrained_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaQxeBiBBkG0"
      },
      "outputs": [],
      "source": [
        "# Assuming train_loader is your DataLoader instance\n",
        "for i, (inputs, labels) in enumerate(val_loader):\n",
        "    print(f\"Batch {i+1}\")\n",
        "\n",
        "    # Print shapes\n",
        "    print(\"Inputs shape:\", inputs.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "    # Print actual data\n",
        "    # Depending on your data, you might need to adjust how you print it\n",
        "    print(\"Inputs data:\", inputs)\n",
        "    print(\"Labels data:\", labels)\n",
        "\n",
        "    if i == 10:  # Inspect the first 2 batches\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "jKx5CzeXnJgI",
        "outputId": "ac120cb1-3469-4966-de3f-764876196a6e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40000, 512])\n",
            "torch.Size([40000, 1])\n",
            "\n",
            "torch.Size([5000, 512])\n",
            "torch.Size([5000, 1])\n",
            "\n",
            "torch.Size([5000, 512])\n",
            "torch.Size([5000, 1])\n"
          ]
        }
      ],
      "source": [
        "print(x_train_padded.shape)\n",
        "print(y_train_tensor.shape)\n",
        "print()\n",
        "print(x_test_padded.shape)\n",
        "print(y_test_tensor.shape)\n",
        "print()\n",
        "print(x_val_padded.shape)\n",
        "print(y_val_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "IY3iuVqjd3Lt",
        "outputId": "3c47fb62-da5c-425c-9b6e-e7ab04b4da88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "train_loss : 0.6934163057804108 val_loss : nan\n",
            "train_accuracy : 50.677499999999995 val_accuracy : 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2\n",
            "train_loss : 0.6932401895999909 val_loss : nan\n",
            "train_accuracy : 50.56 val_accuracy : 0.0\n",
            "Epoch 3\n",
            "train_loss : 0.6895505448818207 val_loss : nan\n",
            "train_accuracy : 51.77 val_accuracy : 0.0\n",
            "Epoch 4\n",
            "train_loss : 0.6452429441213607 val_loss : nan\n",
            "train_accuracy : 56.75750000000001 val_accuracy : 0.0\n",
            "Epoch 5\n",
            "train_loss : 0.33548453809022905 val_loss : nan\n",
            "train_accuracy : 86.2375 val_accuracy : 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "clip = 5\n",
        "epochs = 5\n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [], []\n",
        "epoch_tr_acc,epoch_vl_acc = [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state\n",
        "\n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "        # batch_size = inputs.size(0)\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        model.zero_grad()\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        output = output.view(-1, 1)  # Reshape to [batch_size, 1]\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(output,labels.view(-1, 1))\n",
        "        train_acc += accuracy\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    # for inputs, labels in val_loader:\n",
        "    #         val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "    #         inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    #         output, val_h = model(inputs, val_h)\n",
        "\n",
        "    #         output = output.view(-1, 1)\n",
        "    #         val_loss = criterion(output, labels.float())\n",
        "\n",
        "    #         val_losses.append(val_loss.item())\n",
        "\n",
        "    #         accuracy = acc(output, labels.view(-1, 1))\n",
        "    #         val_acc += accuracy\n",
        "\n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(val_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch+1}')\n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "okyfe6aXELyg",
        "outputId": "9c66751d-88d6-4efd-a47a-3465b3d64978"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.32323430293494726\n",
            "Test accuracy: 87.03999999999999%\n"
          ]
        }
      ],
      "source": [
        "# Function for accuracy calculation\n",
        "def acc(pred, label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n",
        "\n",
        "# Testing loop\n",
        "test_losses = []  # to track the loss\n",
        "test_acc = 0.0    # to track the accuracy\n",
        "\n",
        "model.eval()  # turn off dropout for testing\n",
        "\n",
        "# Iterate over the test set\n",
        "for inputs, labels in test_loader:\n",
        "    # Adjust the batch size based on the current batch\n",
        "    current_batch_size = inputs.size(0)\n",
        "    test_h = model.init_hidden(current_batch_size)\n",
        "\n",
        "    test_h = tuple([each.data for each in test_h])  # detach hidden state\n",
        "\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    output, test_h = model(inputs, test_h)\n",
        "    output = output.view(-1, 1)\n",
        "\n",
        "    test_loss = criterion(output, labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    accuracy = acc(output, labels.view(-1, 1))\n",
        "    test_acc += accuracy\n",
        "\n",
        "# Calculate the average test loss and accuracy\n",
        "avg_test_loss = np.mean(test_losses)\n",
        "avg_test_acc = test_acc / len(test_loader.dataset)\n",
        "\n",
        "# Print the test results\n",
        "print(f'Test loss: {avg_test_loss}')\n",
        "print(f'Test accuracy: {avg_test_acc * 100}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}