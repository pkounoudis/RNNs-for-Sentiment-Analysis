{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "wQ95vIqHH_4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZgtSkovsIBME",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1546adce-9078-4a5a-a1d8-9b7e0de2ad28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3')\n",
        "!pwd"
      ],
      "metadata": {
        "id": "qH2VY7G8IDFu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "706d3e8c-daff-40df-90e5-8b947b77bb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f = open('/content/drive/MyDrive/Panos/Εργασία DeepLearning/Assignment 2 & 3/all_data_IMDB.json')\n",
        "all_data = json.load(f)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3-i7jOKlCSj3",
        "outputId": "68322d25-3c43-4896-a535-bbd824b6d16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(all_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KeOa-_UWEh93",
        "outputId": "e2744a94-71d0-4b9b-def2-596c2b69de26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_samples = []\n",
        "negative_samples = []\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "\n",
        "for item in range(0, len(all_data)):\n",
        "  if all_data[item][1] == 1:\n",
        "    count1 += 1\n",
        "    positive_samples.append(all_data[item][0])\n",
        "  else:\n",
        "    count2 += 1\n",
        "    negative_samples.append(all_data[item][0])\n",
        "\n",
        "print(\"Positive reviews:\", count1)\n",
        "print(\"Negative reviews:\", count2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9bPoTn0gCYaO",
        "outputId": "832fbf28-6c93-4eed-8a15-ddeca36978cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive reviews: 25000\n",
            "Negative reviews: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(negative_samples[0])\n",
        "print(negative_samples[1])\n",
        "print()\n",
        "print(positive_samples[0])\n",
        "print(positive_samples[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "sLACLtoniifQ",
        "outputId": "e95f18f3-645e-4441-d5ae-7ca78b752342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Despite some moments in heavy rain, an encounter with a drunk as well as an organ grinder with a gypsy and a monkey, and a stay in a sanitarium, this Roscoe \"Fatty\" Arbuckle silent comedy short with support from Buster Keaton and Al St. John is only fitfully amusing though there is a quite funny sequence of Arbuckle in drag flirting with Buster that's the ultimate in \"meet cute\" scenes especially since it's one of the few times we see The Great Stone Face smile and laugh in the movies! Also, many scenes seem to have been jump cut edited possibly because of overuse of the film stock. Still, if you're an Arbuckle or Keaton completist, Good Night, Nurse! is certainly worth a look.\n",
            "Without doubt, one of the worst films ever made. Sluggish and without structure, tension or story, the film coasts on the thin premise of \"putting together a show\". Conflicts are resolved within two or three seconds of their inception and dialogue is random and incidental. Everything is put together in a slapdash order and often \"Stepping Out\" feels more like a deleted scenes reel than an actual movie. The film seems to exist merely as a showcase for gaudy and totally random Liza musical numbers. Shelly Winters can be seen in the far superior octo-epic \"Tentacles\", and the REAL Liza can be found in the Showtime release of \"Queer Duck: The Movie\".\n",
            "\n",
            "Guess a few upscale film directors were sitting around sipping their absinthe, grappa, aramangac or jungle juice some night in the 80's during the Cannes or other film festival and one said \"Hey, guys let's do a movie where each of us creates a segment around a world class aria.\" Welllll...it kind of sort of worked. Clearly someone was smart enough to select some of the best recordings of the arias chosen, for example Bjoreling's Nessun Dorma, so if you were blind and lying on the floor just listening to the DVD you got more than your money's worth. Not every director succeeded but more did than not and the flick seems to improve with each viewing over the years. My favorite is the eerily beautiful love duet from Die Todt Statd; okay a young naked Elizabeth Hurley is eye candy but her husband singing to her, his wife's ghost, is incredibly beautiful with the love music second only to Otello and Desdemona's \"Gia nella Notte Densa\" in all the operatic repertoire. Could the flick been better, sure, what couldn't not have been but it's well worth a view especially of you're in a hyper-romantic mood.\n",
            "The penultimate collaboration between director Anthony Mann and star James Stewart (excluding the few days Mann worked on Night Passage before parting company with the star under less than amicable circumstances), The Far Country belies its mainstream look to offer another portrait of an embittered man dragged unwillingly to his own redemption, fighting it every step of the way. This time he's a cattle driver whose response to labour problems - challenging troublesome cowhands to a gunfight at the end of the trail - results in his cattle being confiscated by John McIntire's larcenous judge of the Roy Bean school of law and order. Stealing them back and taking them across the Canadian border, he soon finds himself unwillingly drawn into the growing conflict between prospectors and the judge as he cheats or kills them out of their claims...<br /><br />While it's no great surprise which way Stewart turns at the end, he's a surprisingly callous critter along the way, even using his desire to just be left alone to excuse not warning a group of prospectors of an impending avalanche when he has the chance because it's not his problem. For most of the film there's really only a hair's breadth between him and McIntire, something the judge recognises immediately, revelling in the company of a kindred spirit even as he's genially planning to lynch him. In many ways the townspeople who put their faith in him probably recognise it too - despite their appeals to his dead-and-buried better nature, there's an unspoken acknowledgement that the only person who can stand up to the judge is someone almost as bad as he is.<br /><br />As usual with Mann there's an exceptional use of high country locations, though for once the final showdown takes place on level ground, and the film is almost perfectly cast with strong support from Walter Brennan, Harry Morgan and Ruth Roman (though Corinne Calvert's young romantic interest veers to the irritating). Sadly the great cinematography of the Canadian Rockies is done few favours by a distinctly average DVD transfer, with only the theatrical trailer as an extra.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pos_texts= np.array(positive_samples)\n",
        "neg_texts= np.array(negative_samples)\n",
        "pos_labels=  np.array([1]*len(positive_samples))\n",
        "neg_labels=  np.array([0]*len(negative_samples))\n",
        "\n",
        "pos_dataset = pd.DataFrame({'review': pos_texts, 'label': pos_labels}, columns=['review', 'label'])\n",
        "neg_dataset = pd.DataFrame({'review': neg_texts, 'label': neg_labels}, columns=['review', 'label'])"
      ],
      "metadata": {
        "id": "zCmPCS_fJ4Hv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ef55c0c1-ec9b-414a-b5b4-0c0b9cb6b5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_texts[0])\n",
        "print(neg_texts[0])\n",
        "\n",
        "\n",
        "print(pos_dataset.head(4))\n",
        "print(neg_dataset.head(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "ztMd0fgliYpY",
        "outputId": "1d8caf9d-b515-456c-e44c-47cf461306c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guess a few upscale film directors were sitting around sipping their absinthe, grappa, aramangac or jungle juice some night in the 80's during the Cannes or other film festival and one said \"Hey, guys let's do a movie where each of us creates a segment around a world class aria.\" Welllll...it kind of sort of worked. Clearly someone was smart enough to select some of the best recordings of the arias chosen, for example Bjoreling's Nessun Dorma, so if you were blind and lying on the floor just listening to the DVD you got more than your money's worth. Not every director succeeded but more did than not and the flick seems to improve with each viewing over the years. My favorite is the eerily beautiful love duet from Die Todt Statd; okay a young naked Elizabeth Hurley is eye candy but her husband singing to her, his wife's ghost, is incredibly beautiful with the love music second only to Otello and Desdemona's \"Gia nella Notte Densa\" in all the operatic repertoire. Could the flick been better, sure, what couldn't not have been but it's well worth a view especially of you're in a hyper-romantic mood.\n",
            "Despite some moments in heavy rain, an encounter with a drunk as well as an organ grinder with a gypsy and a monkey, and a stay in a sanitarium, this Roscoe \"Fatty\" Arbuckle silent comedy short with support from Buster Keaton and Al St. John is only fitfully amusing though there is a quite funny sequence of Arbuckle in drag flirting with Buster that's the ultimate in \"meet cute\" scenes especially since it's one of the few times we see The Great Stone Face smile and laugh in the movies! Also, many scenes seem to have been jump cut edited possibly because of overuse of the film stock. Still, if you're an Arbuckle or Keaton completist, Good Night, Nurse! is certainly worth a look.\n",
            "                                              review  label\n",
            "0  Guess a few upscale film directors were sittin...      1\n",
            "1  The penultimate collaboration between director...      1\n",
            "2  I saw this when I was 17 and haven't seen it s...      1\n",
            "3  I really like this film... when I started to w...      1\n",
            "                                              review  label\n",
            "0  Despite some moments in heavy rain, an encount...      0\n",
            "1  Without doubt, one of the worst films ever mad...      0\n",
            "2  Why Lori Petty was cast as tank girl, I'll nev...      0\n",
            "3  You expect it to be juvenile but you at least ...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_train = pos_dataset.sample(frac = 0.8)\n",
        "neg_train = neg_dataset.sample(frac = 0.8)\n",
        "pos_part_20 = pos_dataset.drop(pos_train.index)\n",
        "neg_part_20 = neg_dataset.drop(neg_train.index)"
      ],
      "metadata": {
        "id": "vUYRi-5BKvxv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "72c32351-f3ac-441c-f3a7-ffb095b5e003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_test = pos_part_20.sample(frac = 0.5)\n",
        "neg_test = neg_part_20.sample(frac = 0.5)\n",
        "pos_val = pos_part_20.drop(pos_test.index)\n",
        "neg_val = neg_part_20.drop(neg_test.index)"
      ],
      "metadata": {
        "id": "oo79Ca12Lvcx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "408c4ade-9f32-4b7b-ad58-cb6036d78293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set= pd.concat([pos_train, neg_train], axis=0)\n",
        "test_set=pd.concat([pos_test, neg_test], axis=0)\n",
        "val_set=pd.concat([pos_val, neg_val], axis=0)\n",
        "dataset =pd.concat([train_set, test_set,val_set], axis=0)"
      ],
      "metadata": {
        "id": "af5eKV23LWMZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "11b671e3-da36-496c-b1e2-f58b99ccf4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = train_set.reset_index()\n",
        "test_set = test_set.reset_index()\n",
        "val_set = val_set.reset_index()\n",
        "dataset = dataset.reset_index()"
      ],
      "metadata": {
        "id": "dHiF0XnlNG0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "17b606e6-a6c2-46d3-8d35-4a39ce9a9e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Raw data: ')\n",
        "print('max length =',np.max([len(x) for x in dataset['review']]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset['review']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "mU2I1NNbOljf",
        "outputId": "54049314-7249-4245-b75a-2637fe479705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw data: \n",
            "max length = 13704\n",
            "mean length = 1309.43102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "train_set['review'] = train_set['review'].apply(normalize_text)\n",
        "val_set['review'] = val_set['review'].apply(normalize_text)\n",
        "test_set['review'] = test_set['review'].apply(normalize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "UwAW9NbfSeNM",
        "outputId": "59b24070-5549-4e7f-c13d-f5805b415b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_v2 =pd.concat([train_set, test_set,val_set], axis=0)\n",
        "\n",
        "print('After normalization: ')\n",
        "print('max length =',np.max([len(x) for x in dataset_v2['review']]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset_v2['review']]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "RxGTv6JUTeia",
        "outputId": "6d689bd7-f974-47a2-e730-cae984839d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After normalization: \n",
            "max length = 9164\n",
            "mean length = 812.165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNKHqH6Pj18N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f465d8d4-173d-4559-e359-3fccdafde1b9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After vectorization: \n",
            "max length = 1384\n",
            "mean length = 119.73148\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create the vocabulary from the training dataset\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=30000)\n",
        "tokenizer.fit_on_texts(train_set['review'])\n",
        "vocab_size= len(tokenizer.word_index)+1\n",
        "\n",
        "# Encode the text data as sequences of integers\n",
        "x_train = tokenizer.texts_to_sequences(train_set['review'])\n",
        "x_val = tokenizer.texts_to_sequences(val_set['review'])\n",
        "x_test = tokenizer.texts_to_sequences(test_set['review'])\n",
        "\n",
        "dataset_v3 = x_train + x_val + x_test\n",
        "\n",
        "print('After vectorization: ')\n",
        "print('max length =',np.max([len(x) for x in dataset_v3]))\n",
        "print('mean length =',np.mean([len(x) for x in dataset_v3]))\n",
        "\n",
        "# Pad the sequences to the same length\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=500)\n",
        "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=500)\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('After padding: ')\n",
        "print('max length =',np.max([len(x) for x in x_train]))\n",
        "print('mean length =',np.mean([len(x) for x in x_train]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "GkzNmuUvKRke",
        "outputId": "bfc8f41b-5d93-4a86-9871-a06f661a6c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After padding: \n",
            "max length = 500\n",
            "mean length = 500.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform the label data\n",
        "y_train = le.fit_transform(train_set['label'])\n",
        "y_val = le.transform(val_set['label'])\n",
        "y_test = le.transform(test_set['label'])\n",
        "\n",
        "# Convert the labels to categorical data\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "FrvBtoZUPOZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained word embeddings into a dictionary\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "vocab=tokenizer.sequences_to_texts(x_train)\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "\n",
        "print('unique words: ',vocab_size)\n",
        "\n",
        "# Create an embedding matrix to use in the model\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "COVb5tB1dLoJ",
        "outputId": "4e1d1385-f449-41f7-b144-719c101c128e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words:  81646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GRU model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, input_length=500))\n",
        "model.add(tf.keras.layers.GRU(256))  # Replaced LSTM with GRU\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "SRMswP0nlzXQ",
        "outputId": "ee69ce52-04bb-495d-de20-4d925cea36cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 88s 69ms/step - loss: 0.3555 - accuracy: 0.8388 - val_loss: 0.2413 - val_accuracy: 0.9036\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.1674 - accuracy: 0.9384 - val_loss: 0.2482 - val_accuracy: 0.9016\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 39s 31ms/step - loss: 0.0824 - accuracy: 0.9713 - val_loss: 0.3365 - val_accuracy: 0.8848\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.4118 - val_accuracy: 0.8908\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 36s 28ms/step - loss: 0.0225 - accuracy: 0.9927 - val_loss: 0.4586 - val_accuracy: 0.8846\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.4599 - accuracy: 0.8826\n",
            "Test loss: 0.45991626381874084\n",
            "Test accuracy: 0.8826000094413757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=True))\n",
        "model.add(tf.keras.layers.LSTM(256))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "YHQVB218s0HW",
        "outputId": "68e6cd48-1f34-40d3-8d05-5c029dd2a8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 93s 73ms/step - loss: 0.3750 - accuracy: 0.8346 - val_loss: 0.2808 - val_accuracy: 0.8852\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 50s 40ms/step - loss: 0.2060 - accuracy: 0.9208 - val_loss: 0.2521 - val_accuracy: 0.9026\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.1207 - accuracy: 0.9571 - val_loss: 0.2751 - val_accuracy: 0.9016\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.0623 - accuracy: 0.9797 - val_loss: 0.3558 - val_accuracy: 0.8942\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.0309 - accuracy: 0.9907 - val_loss: 0.4613 - val_accuracy: 0.8886\n",
            "157/157 [==============================] - 2s 11ms/step - loss: 0.4848 - accuracy: 0.8854\n",
            "Test loss: 0.4848434031009674\n",
            "Test accuracy: 0.8853999972343445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN-LSTM model\n",
        "model = tf.keras.Sequential()\n",
        "# Embedding layer\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=True))\n",
        "# Convolutional Layer\n",
        "model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "# LSTM Layer\n",
        "model.add(tf.keras.layers.LSTM(256))\n",
        "# Output Layer\n",
        "model.add(tf.keras.layers.Dense(2, activation='sigmoid'))  # Assuming binary classification\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "id": "iLprFp7TwTLI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "69e57b6f-7e51-419f-ab6a-809dc1a542dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 500, 100)          8164600   \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 496, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 248, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256)               394240    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8623482 (32.90 MB)\n",
            "Trainable params: 8623482 (32.90 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 83s 64ms/step - loss: 0.3589 - accuracy: 0.8384 - val_loss: 0.2559 - val_accuracy: 0.8960\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 40s 32ms/step - loss: 0.1952 - accuracy: 0.9256 - val_loss: 0.2431 - val_accuracy: 0.9052\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.1139 - accuracy: 0.9603 - val_loss: 0.2762 - val_accuracy: 0.9046\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 29s 23ms/step - loss: 0.0520 - accuracy: 0.9834 - val_loss: 0.3866 - val_accuracy: 0.9004\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 28s 22ms/step - loss: 0.0267 - accuracy: 0.9915 - val_loss: 0.4489 - val_accuracy: 0.8854\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.4593 - accuracy: 0.8826\n",
            "Test loss: 0.45930981636047363\n",
            "Test accuracy: 0.8826000094413757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN-GRU model\n",
        "model = tf.keras.Sequential()\n",
        "# Embedding layer\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=True))\n",
        "# Convolutional Layer\n",
        "model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
        "# GRU Layer\n",
        "model.add(tf.keras.layers.GRU(256))  # Replacing LSTM with GRU\n",
        "# Output Layer - Assuming binary classification with 2 classes, using sigmoid activation\n",
        "model.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Model summary to view the architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "zr3J2PJBJDkk",
        "outputId": "310b4deb-7053-49eb-c123-601705380e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 500, 100)          8164600   \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 496, 128)          64128     \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, 248, 128)          0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " gru_3 (GRU)                 (None, 256)               296448    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8525690 (32.52 MB)\n",
            "Trainable params: 8525690 (32.52 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 81s 63ms/step - loss: 0.3330 - accuracy: 0.8513 - val_loss: 0.2476 - val_accuracy: 0.8972\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 37s 30ms/step - loss: 0.1812 - accuracy: 0.9298 - val_loss: 0.2287 - val_accuracy: 0.9078\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 28s 23ms/step - loss: 0.0958 - accuracy: 0.9662 - val_loss: 0.3015 - val_accuracy: 0.8986\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 26s 21ms/step - loss: 0.0379 - accuracy: 0.9876 - val_loss: 0.3928 - val_accuracy: 0.8824\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 25s 20ms/step - loss: 0.0204 - accuracy: 0.9931 - val_loss: 0.4983 - val_accuracy: 0.8968\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.5220 - accuracy: 0.8894\n",
            "Test loss: 0.5220457315444946\n",
            "Test accuracy: 0.8894000053405762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, input_length=500))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.2))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "g9thEic4s5O0",
        "outputId": "be035835-8e88-4abe-d8b8-bb341dd7cf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 94s 72ms/step - loss: 0.3982 - accuracy: 0.7903 - val_loss: 0.3125 - val_accuracy: 0.8754\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 45s 36ms/step - loss: 0.1860 - accuracy: 0.9293 - val_loss: 0.2698 - val_accuracy: 0.8944\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 36s 29ms/step - loss: 0.0996 - accuracy: 0.9645 - val_loss: 0.2733 - val_accuracy: 0.8898\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.0477 - accuracy: 0.9842 - val_loss: 0.3524 - val_accuracy: 0.8836\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 34s 27ms/step - loss: 0.0309 - accuracy: 0.9897 - val_loss: 0.3699 - val_accuracy: 0.8846\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.3895 - accuracy: 0.8764\n",
            "Test loss: 0.38954052329063416\n",
            "Test accuracy: 0.8763999938964844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=True))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.2))\n",
        "model.add(tf.keras.layers.MaxPooling1D(5))\n",
        "model.add(tf.keras.layers.Conv1D(512, 5, activation='relu'))\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "score = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(f'Test loss: {score[0]}')\n",
        "print(f'Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "rSp9ilF8pV-s",
        "outputId": "50bbaad9-7b54-433d-d47a-af232eec7e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1250/1250 [==============================] - 90s 71ms/step - loss: 0.3793 - accuracy: 0.8210 - val_loss: 0.2943 - val_accuracy: 0.8880\n",
            "Epoch 2/5\n",
            "1250/1250 [==============================] - 46s 37ms/step - loss: 0.2129 - accuracy: 0.9165 - val_loss: 0.2583 - val_accuracy: 0.8932\n",
            "Epoch 3/5\n",
            "1250/1250 [==============================] - 37s 29ms/step - loss: 0.1343 - accuracy: 0.9499 - val_loss: 0.2558 - val_accuracy: 0.8954\n",
            "Epoch 4/5\n",
            "1250/1250 [==============================] - 35s 28ms/step - loss: 0.0757 - accuracy: 0.9729 - val_loss: 0.3146 - val_accuracy: 0.8878\n",
            "Epoch 5/5\n",
            "1250/1250 [==============================] - 33s 27ms/step - loss: 0.0445 - accuracy: 0.9851 - val_loss: 0.3155 - val_accuracy: 0.8924\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 0.3482 - accuracy: 0.8844\n",
            "Test loss: 0.3481677174568176\n",
            "Test accuracy: 0.8844000101089478\n"
          ]
        }
      ]
    }
  ]
}